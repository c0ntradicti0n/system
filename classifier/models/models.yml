defaults: &defaults
  embedding_dim: 1024
  batch_size: 2000
  n_epochs: 10000
  stop_f1_score: 0.93
  weight_decay: 0.001
  batches_per_epoch: 1
  embedding_model: BAAI/bge-large-en-v1.5

siamese: &siamese
  <<: *defaults
  model: siamese

classifier: &classifier
  <<: *defaults
  model: ntuple
  lr: 0.00003

som: &som
  <<: *defaults
  model: som
  lr: 0.00003


models:
  # coordinator for self organizing map
  som:
    <<: *som
    n_samples: 5
    n_classes: 9
    # 0 - stay at higher concept
    # 1 - stay at 1
    # 2 - stay at 2
    # 3 - stay at 3
    # 4 - get higher left
    # 5 - get higher right
    # 6 - deeper at 1
    # 7 - deeper at 2
    # 8 - deeper at 3

    batch_size: 1000
    batches_per_epoch: 17
    lr: 0.003

    embedding_dim: 4096
    hidden_dim: 1024


    from_module: classifier.data.triangle_pos

    f1: micro

    classes:
      - from_module
    probs:
      - 1

    embedding_model: Salesforce/SFR-Embedding-Mistral



  thesis_antithesis_synthesis:
    <<: *classifier
    n_samples: 3
    n_classes: 4
    batch_size: 1000
    from_file: classifier.data.thesis_antithesis_synthesis
    batches_per_epoch: 17
    classes:
      - valid
      - from_file
    probs:
      - 0.95
      - 0.05
    relations:
      - syn_1
      - syn_2
  score_hierarchy:
    <<: *siamese
    samples: classifier.data.hierarchy
    inverse: true
    relations:
      - hie
  hierarchy:
    <<: *classifier
    n_classes: 3
    n_samples: 2
    batches_per_epoch: 17
    batch_size: 1000
    classes:
      - valid_hie_wordnet
    probs:
      - 1
    lr: 0.003
    relations:
      - hie

  opposites:
    <<: *siamese
    samples: classifier.data.opposites
    inverse: true
    batch_size: 1000
    n_samples: 2
    relations:
      - ant










